{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODEL = False\n",
    "OUTPUT_PNG  = False\n",
    "VISUALIZE   = False\n",
    "USE_KNN     = False\n",
    "USE_NAIVE   = True\n",
    "V_WIDTH     = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def path_join(arr):\n",
    "    r = arr[0]\n",
    "    for i in range(1, len(arr)):\n",
    "        r = os.path.join(r, arr[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_PNG:\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "\n",
    "    train = np.load('train.npy', allow_pickle=True)\n",
    "    for idx, img in enumerate(train):\n",
    "        img_buf = np.uint8((img + 1) / 2 * 255)\n",
    "        fig = Image.fromarray(img_buf)\n",
    "        fig.save(path_join(['images', 'train', f'{idx}.png']))\n",
    "\n",
    "    test = np.load('test.npy', allow_pickle=True)\n",
    "    for idx, img in enumerate(test):\n",
    "        img_buf = np.uint8((img + 1) / 2 * 255)\n",
    "        fig = Image.fromarray(img_buf)\n",
    "        fig.save(path_join(['images', 'test', f'{idx}.png']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, size=1024):\n",
    "        super(UnFlatten, self).__init__()\n",
    "        self.size = size\n",
    "    def forward(self, input):\n",
    "#         print(input.view(input.size(0), self.size, int((input.size(1) / self.size) ** 0.5), -1).shape)\n",
    "        return input.view(input.size(0), self.size, int((input.size(1) / self.size) ** 0.5), -1)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, z_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 12, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 96, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(75264, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, z_dim)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(75264, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, z_dim)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(z_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 75264)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(96),\n",
    "            nn.ConvTranspose2d(96, 48, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(48, 24, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, image_channels, kernel_size=2, stride=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_().to(DEVICE)\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size()).to(DEVICE)\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        z = self.fc3(z)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def loss_vae(recon_x, x, mu, logvar, criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return mse + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE:\n",
    "    %matplotlib notebook\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    plt.ion()\n",
    "    fig.show()\n",
    "    \n",
    "    def update_line(new_data):\n",
    "        if not hasattr(update_line, \"xs\"):\n",
    "            update_line.xs = []\n",
    "        if not hasattr(update_line, \"ys\"):\n",
    "            update_line.ys = []\n",
    "        ax.clear()\n",
    "        if len(update_line.xs) == V_WIDTH:\n",
    "            update_line.xs = update_line.xs[1:]\n",
    "        if len(update_line.ys) == V_WIDTH:\n",
    "            update_line.ys = update_line.ys[1:]\n",
    "        update_line.xs.append(new_data[0])\n",
    "        update_line.ys.append(new_data[1])\n",
    "        ax.plot(update_line.xs, update_line.ys)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    num_epochs = 200\n",
    "    batch_size = 128\n",
    "    learning_rate = 1e-3\n",
    "    \n",
    "    train = np.load('train.npy', allow_pickle=True)\n",
    "    x = train.reshape(len(train), 3, 32, 32)\n",
    "    \n",
    "    data = torch.tensor(x, dtype=torch.float)\n",
    "    train_dataset = TensorDataset(data)\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    model = VAE().to(DEVICE)\n",
    "#     model = torch.load('best_model_vae.pt').to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    best_loss = np.inf\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        st_time = time.time()\n",
    "        for data in train_dataloader:\n",
    "            img = data[0].to(DEVICE)\n",
    "            # ===================forward=====================\n",
    "            output = model(img)\n",
    "            loss = loss_vae(output[0], img, output[1], output[2], criterion)\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # ===================save====================\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                torch.save(model, 'best_model_vae.pt')\n",
    "        # ===================log========================\n",
    "        if VISUALIZE:\n",
    "            update_line((epoch+1, loss.item()))\n",
    "        ed_time = time.time()\n",
    "        print('epoch [{}/{}], loss:{:.4f}, {:.4f} sec(s)'\n",
    "              .format(epoch + 1, num_epochs, loss.item(), ed_time - st_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3, 32, 32)\n",
      "(40000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "if USE_NAIVE:\n",
    "    from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "    import numpy as np\n",
    "    \n",
    "    test = np.load('train.npy', allow_pickle=True)\n",
    "    y = test.reshape(len(test), 3, 32, 32)\n",
    "    \n",
    "    data = torch.tensor(y, dtype=torch.float)\n",
    "    test_dataset = TensorDataset(data)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=128)\n",
    "\n",
    "    model = torch.load('best_model_vae.pt').to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    reconstructed = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            output = model(data[0].to(DEVICE))\n",
    "            output = output[0]\n",
    "            reconstructed.append(output.cpu().numpy())\n",
    "\n",
    "    reconstructed = np.concatenate(reconstructed, axis=0)\n",
    "    \n",
    "    print(reconstructed.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    anomality = np.sqrt(np.sum(np.square(reconstructed - y).reshape(len(y), -1), axis=1))\n",
    "    y_pred = anomality\n",
    "    with open('prediction.csv', 'w') as f:\n",
    "        f.write('id,anomaly\\n')\n",
    "        for i in range(len(y_pred)):\n",
    "            f.write('{},{}\\n'.format(i+1, y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_KNN:\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    from sklearn.metrics import f1_score, pairwise_distances, roc_auc_score\n",
    "    from scipy.cluster.vq import vq, kmeans\n",
    "    import numpy as np\n",
    "\n",
    "    def vae_encode(datas):\n",
    "        model = torch.load('best_model_vae.pt').to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        codes = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            data = torch.tensor(datas, dtype=torch.float)\n",
    "            dataset = TensorDataset(data)\n",
    "\n",
    "            dataloader = DataLoader(dataset, batch_size=128)\n",
    "            for data in dataloader:\n",
    "                img = data[0].to(DEVICE)\n",
    "                _, mu, lvar = model(img)\n",
    "                if len(codes) == 0:\n",
    "                    codes = model.reparametrize(mu, lvar).cpu().numpy()\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, model.reparametrize(mu, lvar).cpu().numpy()))\n",
    "        return codes\n",
    "\n",
    "    test = np.load('test.npy', allow_pickle=True)\n",
    "    \n",
    "    y = test.reshape(len(test), -1)\n",
    "    y_code = vae_encode(y)\n",
    "\n",
    "    kmeans_y = MiniBatchKMeans(n_clusters=10, batch_size=1024).fit(y_code)\n",
    "    y_cluster = kmeans_y.predict(y_code)\n",
    "    y_dist = np.sum(np.square(kmeans_y.cluster_centers_[y_cluster] - y_code), axis=1)\n",
    "\n",
    "    y_pred = y_dist\n",
    "    with open('prediction.csv', 'w') as f:\n",
    "        f.write('id,anomaly\\n')\n",
    "        for i in range(len(y_pred)):\n",
    "            f.write('{},{}\\n'.format(i+1, y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAADeElEQVR4nF2WQbLkSAhDn+i89CxmMQdvNAsB/t2OiqooO02ChETq3//+AQnAkpg/96Uq2SaXED+ulmXm8R8/va88jN2eNw3CUN4d5aaxAAths5sYi0RjkzDGxrSsxvVwg20luhLny9i2pEQ1nsgT/mLeI2By79x7d1t5pizVbmbNKxfg23vL8e66C7/1/bJA9hQ+zyq7Ved3Y+4KLSZ2X07+OxELPbDdmji2kbB/gyRsCTCSgokxSqwfiXvhV/5p+JRfKBlwpsAhwY3KzUE1C5HJZgpO12RHkg2SbD2w3MfYrOphpa3Cvn75kMaW3TWYLxNLcTBU9RtsO6/MWo7AVpLptmopkKDxL8kYIdTDCKleuClhPdzIwsJufeVPd17h6t9IRAsCqWl3FrYJSddKCqr9BN1pSaROgwKoQWpHJWJJODGH59A98vmBH4KW9Nqmf3t0SfrGI82E7q9Hh35tbFnpaBOYPzISTk+HghkX6NDsMgL99CIv1cOWmb7WVDZpzlaCh3w60STqhQlN7097nhU5rZxg2voj0A2SXZ888hmMt+PSPV/K192D7wpMpgvJDrEHxPTLA01R9njGVWiv/j79e/vevRnFm6N1zp0cy38aeaY1Z6cpojftsYyzTLsjVFzGdWwwOlfkD34GUqHAfUofKHr2VG/uW4d3PIh2i5Lo0BghaCC6vvEJmPE+2x8Jv7W2OfPjB3XU2ElgsKyaXqvpOueJ7fX5MxfGpxo3NJ+Rptss2/ZOmG9aje9Pi2Z9/QS6oTW8x7Y/tpU5kP+y9Pndjg9VCZVAmnE/TZq88izT2djhR/7iGkklaxOoPF87cQuV6uYB7EjJ1XRGp4v4nEd7Sujzjz6ZXI15cSFCg8AmMGrUTglZ85lpfeZngOCwMUgCsfX32eOdJsaS5mR0e/xxIlKOBBdy7vnHupT6VhNiHTyvzUllJp9nDCe1VZOu9Mzfk5emCnLwoqQOz0Yl/5lwLGyUPwM6MRKrbJe+uvYrybxYbZIfVxLemT59ZH0VsWki3VlTaFOSEMV672OTMXJ5Qf01zuzpwrP5GX3aIvf2sbalOD/vMK4/SJwMAm3vVNjtAVE5OJwCd4YPDDNf3jTLVTjtyZwwhE2JMs255cYQS87qyKqKjanKFg+XqjPhw9vYrQyU1eUIpvj7Cgxds+PoagYtdGP+B5JDGJhJslp1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x21EDE5E68D0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in test_dataloader:\n",
    "#         output = model(data[0].to(DEVICE))\n",
    "#         output = output[0]\n",
    "#         break\n",
    "\n",
    "Image.fromarray(np.uint8((reconstructed[0].reshape(-1).reshape((32,32,3)) + 1)/2*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
